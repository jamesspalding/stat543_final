---
title: "Predicting Poverty"
author: "James Spalding, Ben Bronoski, Matt Nowell, Yaya Barrow"
date: "`r Sys.Date()`"
output:
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = F, include = F, message = F)

library(tidyverse)
library(VGAM)
library(nnet)
library(pROC)
library(caret)
library(glmnet)
library(gt)
library(gtExtras)

select = dplyr::select
```
# Introduction

There is a constant struggle to ensure people are being given access to the correct amount of aid they need to survive. Some programs target some of the poorest populations to ensure they are being properly taken care of. Unfortunately, some of these poorer communities are unable to correctly and accurately document that they qualify for the amount of aid they tend to need. The goal of this project is to take observable attributes of a given household and bucket them into different poverty levels.

The data provided includes 142 predictor variables, with a decent spread between categorical, numeric, and binary values. Our response, **Target**, is a categorical variable with 4 levels, and each row represents an observed individual. Below is a list of variables with some omitted or modified for clarity:

```{r, include=T}
#make a nice GT table with a smaller list of vars
Variable =    c('v2a1',
                'hacdor',
                'rooms',
                'hacapo',
                'v14a',
                'refrig',
                'v18q1', 
                'r4t1',
                'r4t2',
                'escolari',
                'rez_esc',
                'hhsize',
                'pared',
                'piso',
                'techo',
                'cielorazo',
                'abastagua',
                'elec',
                'sanitario',
                'energcocinar',
                'elimbasu',
                'epared',
                'etecho',
                'eviv',
                'dis',
                'gender',
                'estadocivil',
                'parentesco',
                'hogar_nin',
                'hogar_adul',
                'hogar_mayor',
                'dependency',
                'edjife',
                'edjifa',
                'meaneduc',
                'instlevel',
                'bedrooms',
                'tipovivi',
                'computer',
                'television',
                'lugar',
                'area',
                'age',
                'Target'
                )


Type =        c('Numeric',
                'Numeric',
                'Numeric',
                'Numeric',
                'Binary',
                'Binary',
                'Numeric',
                'Numeric',
                'Numeric',
                'Numeric',
                'Numeric',
                'Numeric',
                'Categorical',
                'Categorical',
                'Categorical',
                'Binary',
                'Categorical',
                'Categorical',
                'Categorical',
                'Categorical',
                'Categorical',
                'Numeric',
                'Numeric',
                'Numeric',
                'Binary',
                'Binary',
                'Categorical',
                'Categorical',
                'Numeric',
                'Numeric',
                'Numeric',
                'Numeric',
                'Numeric',
                'Numeric',
                'Numeric',
                'Categorical',
                'Numeric',
                'Categorical',
                'Binary',
                'Binary',
                'Categorical',
                'Binary',
                'Numeric',
                'Categorical'
                )


Description = c('Monthly rent payment',
                'Overcrowding by bedrooms',
                'Number of rooms in house',
                'Overcrowding by all rooms',
                'Has bathroom in household',
                'Has refrigerator in household',
                'Number of tablets household owns',
                'Persons younger than 12 years',
                'Persons older than 12 years',
                'Years of schooling',
                'Years behind in school',
                'Household size',
                'Wall material',
                'Floor material',
                'Roof material',
                'Presence of ceiling in home',
                'Home water source',
                'Home electricity source',
                'Home plumbing type',
                'Home kitchen type',
                'Home waste disposal type',
                'Wall quality',
                'Roof quality',
                'Floor quality',
                'Individual is disabled',
                'Individual gender',
                'Individual civil status',
                'Relation to head of household',
                'Individuals under 19',
                'Individuals between 19 and 65',
                'Individuals >65',
                'Ratio of dependents/independents',
                'Education of head of household (male)',
                'Education of head of household (female)',
                'Mean years of education in household',
                'Highest form of education achieved',
                'Number of bedrooms',
                'House status (rent, own, etc)',
                'Presence of household computer',
                'Presence of household TV',
                'Region',
                'Urban/Rural',
                'Individual age',
                'Household poverty level'
                )


as.data.frame(t(as.data.frame(rbind(Variable,Type,Description)))) %>%
  gt() %>%
  gt_theme_nytimes()
```

Furthermore, our response variable **Target** has the following categories which we will attempt to classify households into:

```{r, include=T}
Level = c(1,2,3,4)

Description = c('Extreme poverty',
                'Moderate poverty',
                'Vulnerable',
                'Non-vulnerable')

#table(data$Target)

Count = c(211,420,339,1849)

as.data.frame(t(as.data.frame(rbind(Level,Description,Count)))) %>%
  gt() %>%
  gt_theme_nytimes()
```

As shown by the counts, this is a very imbalanced dataset and measures will need to be taken to account for this.

# Data Cleaning and Exploration

There are several variables, such as SQBescolari, SQBage, SQBhogar_total, SQBedjefe, SQBhogar_nin, SQBovercrowding, SQBdependency, SQBmeaned, agesq, that are either irrelevant in the analysis, or hold little value to the outcome of the analysis. These variables are squares of other existing variables so they are removed from the dataset to avoid any colinearity. Additional data cleaning steps address missing values, standardize data formats, and compute new variables to ensure the dataset is ready for analysis. Missing values are filled with logical defaults or calculated averages. One example of this is replacing v2a1 (rent payment) and v18q1 (tablets owned) with zero, or using the mean education level for specific age groups to impute meaneduc. Categorical variables, like edjefa and edjefe, are converted into binary numeric formats. Dependency ratios are recalculated at the household level using age group distributions, and a binary indicator for school attendance is created.

Once these steps have been executed, new variables are made to address the ambiguity of some of the existing features. Some of these variables create household-level summaries, such as counts of individuals in school, children behind in school, and disabled members, and identifies households with non-family members. A filter is applied to the dataset to include only heads of households and removes redundant or constant features. Housing quality is quantified through composite scores for interior, exterior, and overall quality based on materials and utilities. To visualize household dependencies, a simple scatterplot was created.

```{r}
data_cleaner = function(data){
  #remove redundant variables
  data = data %>% select(-c(SQBescolari, SQBage, SQBhogar_total, SQBedjefe,
                            SQBhogar_nin, SQBovercrowding, SQBdependency, SQBmeaned,
                            agesq))
  
  
  
  ### Fill NAs ###
  #colSums(is.na(data))
  
  #6405 v2a1: rent payment
  house_types = cbind(data$v2a1, data$tipovivi1, data$tipovivi2,
                      data$tipovivi3, data$tipovivi4, data$tipovivi5)
  
  colSums(is.na(as.data.frame(house_types) %>% filter(data$tipovivi5 == 1))) 
  #5600 tipovivi1: owned
  #153 tipovivi4: precarious
  #751 tipoivi5: assigned
  
  #assign 0 to NA in v2a1
  data$v2a1[is.na(data$v2a1)] = 0
  
  
  #v18q1 tablets owned:
  tabs = cbind(data$v18q, data$v18q1)
  # NA if zero, so replace with 0.
  data$v18q1[is.na(data$v18q1)] = 0
  
  
  #rez_esc: years behind in school
  school = as.data.frame(cbind(data$rez_esc, data$age))
  school_not_na = school %>% filter(!is.na(school$V1))
  #applies only to those in school (7-17)
  #fill with 0
  #in school binary
  data$in_school = as.numeric(!is.na(data$rez_esc))
  data$rez_esc[is.na(data$rez_esc)] = 0
  
  
  #meaneduc: avg edu for adults
  age18 = data %>% filter(age == 19 | age == 18)
  mean(na.omit(age18$meaneduc)) #13.66
  mean(na.omit(data$meaneduc)) #9.23
  #since small portion (5 obs), fill with mean for age (13.66)
  data$meaneduc[is.na(data$meaneduc)] = 13.66
  
  
  ### Standardize types ###
  #sapply(data, class) 
  
  #edjefe/edjefa
  #yes=1, no=0
  data$edjefa[data$edjefa == 'no'] = 0
  data$edjefa[data$edjefa == 'yes'] = 1
  data$edjefa = as.numeric(data$edjefa)
  
  data$edjefe[data$edjefe == 'no'] = 0
  data$edjefe[data$edjefe == 'yes'] = 1
  data$edjefe = as.numeric(data$edjefe)
  
  
  # #dependancy: (number of members of the household younger than 19 or older than 64)/(number of member of household between 19 and 64)         
  table(data$dependency)
  
  #calculate for missing vals (0 if none)
  dependency_calc = data %>%
    group_by(idhogar) %>%
    summarise(count_in_range = sum(age >= 19 & age <= 64),
              count_out_range = sum(!(age >= 19 & age <= 64)),
              ratio = ifelse(count_out_range == 0, 0, count_in_range / count_out_range))
  
  result = inner_join(data, dependency_calc, by = "idhogar")
  result = result %>% select(-c(count_in_range, count_out_range,dependency))
  result$dependency = result$ratio
  data = result %>% select(-c(ratio))
  
  
  
  ##### New Variables #####
  
  #new var for number of people in school by household
  school_count = data %>%
    group_by(idhogar) %>%
    summarise(school_count = sum(in_school == 1))
  
  data = inner_join(data, school_count, by = "idhogar")
  
  
  #count of children behind in school
  children_behind = data %>%
    group_by(idhogar) %>%
    summarise(children_behind = sum(rez_esc != 0))
  
  data = inner_join(data, children_behind, by = "idhogar")
  
  
  #count of disabled people in house
  disabled_count = data %>%
    group_by(idhogar) %>%
    summarise(dis_count = sum(dis == 1))
  
  data = inner_join(data, disabled_count, by = "idhogar")
  
  
  #has non-family in household binary
  non_fam = data %>%
    group_by(idhogar) %>%
    summarise(non_family = as.numeric(any(parentesco12 != 0)))
  
  data = inner_join(data, non_fam, by = "idhogar")
  
  
  #split ids
  numeric_data = data %>% select(-c(Id,idhogar,Target))
  
  #check correlations (spearman for nonlinear)
  # corrs = cor(numeric_data, method = 'spearman') %>%
  #           as.data.frame() %>%
  #           mutate(var1 = rownames(.)) %>%
  #           gather(var2, value, -var1) %>%
  #           arrange(desc(value)) %>%
  #           group_by(value) %>%
  #           filter(row_number()==1)
  # 
  # corrs = corrs[corrs$value >= .7 | corrs$value <= -.7,]
  
  #tamhog, tamviv, r4t3 basically same info
  # male/female, area1/area2, and abast... should be binary
  
  #remove more redundant data
  data = data %>% select(-c(female, area2, abastaguafuera, tamhog, tamviv, v18q,in_school))
  
  
  #which vars are not the same within households
  # independent_vars = data %>%
  #   group_by(idhogar) %>%
  #   summarize(across(everything(), ~ n_distinct(.) > 1)) %>%
  #   select(-idhogar) %>%
  #   summarize(across(everything(), any)) %>%
  #   pivot_longer(cols = everything(), names_to = "Column", values_to = "NotConstant") %>%
  #   filter(NotConstant) %>%
  #   pull(Column)
  # 
  # print(independent_vars)
  
  
  ##### Head of Household Data ####
  
  data_hh = data %>%
    filter(parentesco1 == 1) %>%
    select(-c(r4h1,r4h2,r4h3,r4m1,r4m2,r4m3,r4t2,r4t3,
              mobilephone,edjefe,edjefa,dis, #redundant
              parentesco1,parentesco2,parentesco3,parentesco4,parentesco5,
              parentesco6,parentesco7,parentesco8,parentesco9,parentesco10,
              parentesco11,parentesco12, #all are in parentesco1, so useless
              elimbasu5, estadocivil1, #all zero
              Id, idhogar
              ))
  
  #which(colSums(data_hh) == 0) #remove 0s
  
  ### House quality variable ###
  
  #pisonotiene: no floor
  #pareddes: waste walls
  #cielorazo=0: no ceiling
  #noelec
  #sanitario1: no toilet
  #energcocinar1: no kitchen
  #refrig=0: no refridgerator
  
  #epared1-3: wall quality
  #etecho1-3:roof quality
  #eviv: floor quality
  
  house_vars = data_hh %>%
    select(c(pisonotiene,pareddes,cielorazo,noelec,sanitario1,refrig,
             energcocinar1,epared1,epared2,epared3,etecho1,etecho2,
             etecho3,eviv1,eviv2,eviv3))
  
  #flip binary to make same as others
  house_vars$refrig = as.numeric(house_vars$refrig == 0)
  house_vars$cielorazo = as.numeric(house_vars$cielorazo == 0)
  
  #exterior vars
  house_vars$wall_qual = ifelse(house_vars$epared1 == 1, 0,
              ifelse(house_vars$epared2 == 1, .5,
              ifelse(house_vars$epared3 == 1, 1,999)))
  
  house_vars$roof_qual = ifelse(house_vars$etecho1 == 1, 0,
              ifelse(house_vars$etecho2 == 1, .5,
              ifelse(house_vars$etecho3 == 1, 1,999)))
  
  house_vars$floor_qual = ifelse(house_vars$eviv1 == 1, 0,
              ifelse(house_vars$eviv2 == 1, .5,
              ifelse(house_vars$eviv3 == 1, 1,999)))
  
  house_vars$exterior_qual = house_vars %>%
    select(roof_qual,wall_qual,floor_qual) %>%
    rowMeans()
  
  #interior vars
  house_vars$interior_qual = house_vars %>%
    select(pisonotiene,pareddes,cielorazo,noelec,sanitario1,energcocinar1,refrig) %>%
    rowMeans()
  
  house_vars$interior_qual = 1-house_vars$interior_qual
  
  
  house_vars = house_vars %>% select(-c(epared1,epared2,epared3,
                                        etecho1,etecho2,etecho3,
                                        eviv1,eviv2,eviv3))
  
  house_vars$house_qual = house_vars %>%
    select(interior_qual, exterior_qual) %>%
    rowMeans()
  
  #put back in full dataset
  data_hh$interior_qual = house_vars$interior_qual
  data_hh$exterior_qual = house_vars$exterior_qual
  data_hh$house_qual = house_vars$house_qual
  
  return(data_hh)
}

data = data_cleaner(read.csv('poverty.csv'))
test_data = data_cleaner(read.csv('poverty-test-blinded.csv')) #works on test data.
```

```{r, include=T,fig.dim = c(8, 4)}
ggplot(data, aes(x = dependency, y = house_qual)) +
  geom_point(alpha = 0.6, color = "blue") + 
  geom_smooth(method = "lm", color = "red", se = TRUE) +
  labs(
    title = "Scatterplot of Dependency vs House Quality",
    x = "Dependency Ratio",
    y = "House Quality Score"
  ) +
  theme_minimal()
```

As we can see from this plot, as dependency ratio increases, the house quality seems to have a slightly larger spread which could indicate that having more dependents could have an impact on the quality of a household. Another argument that could be made is that people that have more dependents appear to be in a position to provide for these dependents since there seems to be an upward trend of the house quality score as the dependency ratio increases. Additionally, there could be a relationship between the rent paid and the house quality. To view this, another scatterplot was made.

```{r,include=T,fig.dim = c(8, 4)}
ggplot(aes(x = v2a1, y = house_qual, color = dependency), data = data) +
  geom_point() +
  labs(x = "Rent Amount", y = "House Quality", title = "Rent Payment vs House Quality", color = "Dependency Ratio") +
  theme_minimal()
```

As we can see in this plot, there seems to be some sort of relationship when rent goes up, the house quality rises with it. There also seems to be a lower dependency ratio as both rent and house quality increase.

# Base Model

```{r}
#split train/test
set.seed(543)
train_ind = sample.int(dim(data)[1], dim(data)[1] * .7)
data_tr = data[train_ind,]
data_te = data[-train_ind,]

#base model
mod0 = multinom(Target~., data=data_tr)
probs = predict(mod0, data_te, type = "probs")
preds = predict(mod0, data_te, type = "class")

confusionMatrix(as.factor(preds), as.factor(data_te$Target)) #already decent
print(auc(multiclass.roc(as.numeric(data_te$Target), as.numeric(preds))))

#test various class weights
#Props: 8%, 16%, 12%, 64%

test_weight = function(class_weights){
  weighted_probs = sweep(probs, 2, class_weights, `*`)
  weighted_probs = sweep(weighted_probs, 1, rowSums(weighted_probs), `/`)
  adjusted_preds = colnames(weighted_probs)[apply(weighted_probs, 1, which.max)]
  print(confusionMatrix(as.factor(adjusted_preds), as.factor(data_te$Target)))
  print(auc(multiclass.roc(as.numeric(data_te$Target), as.numeric(adjusted_preds))))
}

inv_prop = c(Class1 = 1/.08, Class2 = 1/.16, Class3 = 1/.12, Class4 = 1/.64)
test_weight(inv_prop)

inv_sqrt = c(Class1 = 1/sqrt(.08), Class2 = 1/sqrt(.16), Class3 = 1/sqrt(.12), Class4 = 1/sqrt(.64))
test_weight(inv_sqrt)
```

Our first step is to get a baseline model. Since our response is categorical with 4 levels, we will be using a multinomial logistic regression model. We separated  our data with a 70% train/test split and created a model using all variables created and retained in the data cleaning process. The performance of this model is shown below:

| **Base Model**  | 1    | 2    | 3    | 4    |
|-------------|------|------|------|------|
| Sensitivity | 0.22 | 0.31 | 0.05 | 0.91 |
| Specificity | 0.96 | 0.90 | 0.97 | 0.46 |

As shown, the initial model performs quite well in class 4, decent in classes 1 and 2, and terribly in class 3. 

```{r,eval=F}
library(glmnet)
#Convert training data to matrix format, required for the package
xx <- model.matrix(Target~., data_tr)
yy <- data_tr$Target #packages requires this to be numeric

#Fit the lasso logistic regression
#alpha=1 gives lasso (alpha=0 gives ridge)
#Can change the model measurement. Using AUC here:

thresh = seq(from = 0, to = 1, by = 0.05)
for(t in thresh){
  lasso.out <- cv.glmnet(xx, yy, family="multinomial", alpha=t, type.measure="class")
  coef(lasso.out, s=lasso.out$lambda.min)
  
  probs = predict(lasso.out, model.matrix(Target~., data_te),type='response')
  preds = predict(lasso.out, model.matrix(Target~., data_te),type='class')
  #table(data_te$Target)
  print(t)
  print(table(preds))
}

#all elastic net are not good.
```

To improve our model, we first attempted elastic net with crossfold validation (10 folds). We initially attempted ridge regression, but noticed that the model was only predicting cases from levels 2 and 4; the two largest groups within the data. We next tried LASSO regression, and had nearly identical results. Finally, in order to rule out elastic net as a viable strategy for our data, we tested every $\alpha$ value between 0 and 1 in intervals of 0.05. Out of these 20 tests, $\alpha=0.3$ was able to make predictions on levels 1, 2, and 4, $\alpha=0.7$ was able to make predictions on all 4 classes, and all others were only able to predict on 2 and 4. However, neither $\alpha=0.3$ nor $\alpha=0.7$ were able to make accurate predictions, so none of the elastic net models will be used.

We hypothesized that, since the predictors appeared to not be a limiting factor, the class imbalance must be the reason for poor predictions. To account for this, we tested a few different weights on each of the class probability predictions. The weights, along with the performance of their corresponding models, are shown below:

| Weight                         | $\kappa$ | AUC  |
|--------------------------------|----------|------|
| None                           | 0.26     | 0.65 |
| $\frac{1}{\text{prop}}$        | 0.27     | .070 |
| $\frac{1}{\sqrt{\text{prop}}}$ | 0.31     | 0.68 |

As shown, both of these models perform above the unweighted model in both $\kappa$ and ROC-AUC scores. The trade off, however, is that neither of these models are nearly as accurate at predicting class 4 as the unweighted version.

# Multi-Model Approach

Since our weighted model did so much better in predicting classes 1-3, we will continue to use it. However, to improve on its lacking capabilities in predicting class 4, we will introduce a second model which *exclusively* predicts class 4.

For this model, the Target variable has been transformed to a binary classifier as to whether the case falls into class 4 or not. In this model, we want as high of an specificity as we can reasonably get, since false positives won't get a second chance at being properly classified by model 2. Since recall (the amount of true positives divided by the total predicted positives) appears to scale linearly with threshold, we took the highest threshold with an acceptable $\kappa$ and AUC score. This value ended up being 0.75. The results are shown in the plot below:

```{r}
##### Binary class 4 predictor #####
#get true/false if in group 4
data_tr_4 = data_tr
data_tr_4$Target = data_tr$Target == 4

data_te_4 = data_te
data_te_4$Target = data_te$Target == 4

#fit model
mod_4 = multinom(Target ~ ., data = data_tr_4)
probs_4 = predict(mod_4, data_te_4, type = "probs")

#test thresh
is = seq(from = 0, to = 1, by = 0.05)
ks = c()
aucs = c()
recs = c()
for(i in is){
  preds_4 = probs_4 > i
  
  conf_mat = confusionMatrix(as.factor(preds_4),as.factor(data_te_4$Target))
  ks = c(ks, conf_mat$overall['Kappa'])
  aucs = c(aucs, auc(roc(as.numeric(data_te_4$Target), as.numeric(preds_4))))
  recs = c(recs, conf_mat$byClass["Recall"])
}

results = data.frame(cbind(is,ks,aucs,recs))
results$ratio = results$ks/results$aucs

#use .75
preds_4 = probs_4 > .75
confusionMatrix(as.factor(preds_4), as.factor(data_te_4$Target))
```

```{r, include = T}
results2 = gather(results, key = "var", value = "y", ks:recs)

ggplot(results2, aes(x=is, y=y, colour=var))+
  geom_line(linewidth = 1.2)+
  geom_vline(xintercept = .75,linetype='dashed')+
  labs(x='Threshold', y='Value')+
  scale_color_discrete(labels = c("AUC", "Kappa", "Recall"))+
  theme_minimal() +
  theme(legend.title = element_blank())
```

Now that we have a threshold selected for the binary model, all we need to do is pass the values that it predicts are not 4 to the original model. The results are shown in the following table:

```{r}
##### Combine models #####
final_preds = sapply(1:nrow(data_te), function(i) {
  # bin 4/not 4
  is_4_probs = predict(mod_4, newdata = data_te[i, ], type = "probs")
  is_4 = is_4_probs > .75
  
  if (is_4 == T) {
    return(4)
  }
  
  # predict on rest
  else {
    not_4_probs = matrix(predict(mod0, newdata = data_te[i, ], type = 'probs'),nrow = 1)
    class_weights = c(Class1=1/.08,Class2=1/.16,Class3=1/.12,Class4=1/.64)

    weighted_probs = sweep(not_4_probs, 2, class_weights, `*`)
    weighted_probs = sweep(weighted_probs, 1, rowSums(weighted_probs), `/`)
    not_4_pred = which.max(weighted_probs)
    
    return(not_4_pred)
  }
})

confusionMatrix(as.factor(final_preds), as.factor(data_te$Target))
```

| **Final Model**  | 1    | 2    | 3    | 4    |
|-------------|------|------|------|------|
| Sensitivity | 0.50 | 0.30 | 0.30 | 0.68 |
| Specificity | 0.88 | 0.87 | 0.84 | 0.80 |

While the sensitivity for observations in class 4 may have fallen by a decent margin, its specificity rose to compensate. While metrics for class 2 predictions are more or less the same, metrics for class 1 and 3 predictions have seen substantial improvements. 

# Conclusion