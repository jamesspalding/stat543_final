---
title: "Predicting Poverty"
author: "James Spalding, Ben Bronoski, Matt Nowell, Yaya Barrow"
date: "`r Sys.Date()`"
output:
  html_document:
    df_print: paged
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = F, include = F, message = F)

library(tidyverse)
library(VGAM)
library(nnet)
library(pROC)
library(caret)
library(glmnet)
library(gt)
library(gtExtras)

select = dplyr::select
```
# Introduction

There is a constant struggle to ensure people are being given access to the correct amount of aid they need to survive. Some programs target some of the poorest populations to ensure they are being properly taken care of. Unfortunately, some of these poorer communities are unable to correctly and accurately document that they qualify for the amount of aid they tend to need. The goal of this project is to take observable attributes of a given household and bucket them into different poverty levels.

The data provided includes 142 predictor variables, with a decent spread between categorical, numeric, and binary values. Our response, **Target**, is a categorical variable with 4 levels, and each row represents an observed individual. Below is a list of variables with some omitted or modified for clarity:

```{r, include=T}
#make a nice GT table with a smaller list of vars
Variable = c('v2a1','hacdor','rooms','hacapo','v14a','refrig','v18q1','r4t1',
             'r4t2','escolari','rez_esc','hhsize','pared','piso','techo',
             'cielorazo','abastagua','elec','sanitario','energcocinar',
             'elimbasu','epared','etecho','eviv','dis','gender','estadocivil',
             'parentesco','hogar_nin','hogar_adul','hogar_mayor','dependency',
             'edjife','edjifa','meaneduc','instlevel','bedrooms','tipovivi',
             'computer','television','lugar','area','age','Target')


Type = c('Numeric','Numeric','Numeric','Numeric','Binary','Binary','Numeric',
         'Numeric','Numeric','Numeric','Numeric','Numeric','Categorical',
         'Categorical','Categorical','Binary','Categorical','Categorical',
         'Categorical','Categorical','Categorical','Numeric','Numeric',
         'Numeric','Binary','Binary','Categorical','Categorical','Numeric',
         'Numeric','Numeric','Numeric','Numeric','Numeric','Numeric',
         'Categorical','Numeric','Categorical','Binary','Binary','Categorical',
         'Binary','Numeric','Categorical')


Description = c('Monthly rent payment','Overcrowding by bedrooms',
                'Number of rooms in house','Overcrowding by all rooms',
                'Has bathroom in household','Has refrigerator in household',
                'Number of tablets household owns',
                'Persons younger than 12 years','Persons older than 12 years',
                'Years of schooling','Years behind in school','Household size',
                'Wall material','Floor material','Roof material',
                'Presence of ceiling in home','Home water source',
                'Home electricity source','Home plumbing type',
                'Home kitchen type','Home waste disposal type','Wall quality',
                'Roof quality','Floor quality','Individual is disabled',
                'Individual gender','Individual civil status',
                'Relation to head of household','Individuals under 19',
                'Individuals between 19 and 65','Individuals >65',
                'Ratio of dependents/independents',
                'Education of head of household (male)',
                'Education of head of household (female)',
                'Mean years of education in household',
                'Highest form of education achieved','Number of bedrooms',
                'House status (rent, own, etc)',
                'Presence of household computer','Presence of household TV',
                'Region','Urban/Rural','Individual age',
                'Household poverty level')


as.data.frame(t(as.data.frame(rbind(Variable,Type,Description)))) %>%
  gt() %>%
  gt_theme_nytimes()
```

Furthermore, our response variable **Target** has the following categories which we will attempt to classify households into:

```{r, include=T}
Level = c(1,2,3,4)

Description = c('Extreme poverty','Moderate poverty',
                'Vulnerable','Non-vulnerable')

#table(data$Target)

Count = c(211,420,339,1849)

as.data.frame(t(as.data.frame(rbind(Level,Description,Count)))) %>%
  gt() %>%
  gt_theme_nytimes()
```

As shown by the counts, the data is rather imbalanced within the levels of vulnerability will be accounted for in future processes.

# Data Cleaning and Exploration

There are several variables, such as SQBescolari, SQBage, SQBhogar_total, SQBedjefe, SQBhogar_nin, SQBovercrowding, SQBdependency, SQBmeaned, agesq, that we deemed irrelevant to our analysis. These variables are squares of other existing variables so they were removed from the dataset to avoid colinearity with their non-square counterparts. Additional data cleaning steps address missing values, standardize data formats, and compute new variables to ensure the dataset is ready for analysis. Missing values are filled with logical defaults or calculated averages. One example of this is replacing v2a1 (rent payment) and v18q1 (tablets owned) with zero, or using the mean education level for specific age groups to impute meaneduc. Categorical variables, like edjefa and edjefe, are converted into binary numeric formats. Dependency ratios are recalculated at the household level using age group distributions, and a binary indicator for school attendance is created.

Once these steps have been executed, new variables were constructed to address the ambiguity of some of the existing features. Some of these variables create household-level summaries, such as counts of individuals in school, children behind in their schooling, and disabled household members, and identifies households with non-family members. A filter is applied to the dataset to include only heads of households and removes redundant or constant features. Housing quality is quantified through composite scores for interior, exterior, and overall quality based on materials and utilities. To visualize household dependencies, a simple scatterplot was created.

```{r}
data_cleaner = function(data){
  #remove redundant variables
  data = data %>% select(-c(SQBescolari, SQBage, SQBhogar_total, SQBedjefe,
                            SQBhogar_nin, SQBovercrowding, SQBdependency, 
                            SQBmeaned,agesq))
  
  ### Fill NAs ###
  #colSums(is.na(data))
  
  #6405 v2a1: rent payment
  house_types = cbind(data$v2a1, data$tipovivi1, data$tipovivi2,
                      data$tipovivi3, data$tipovivi4, data$tipovivi5)
  
  colSums(is.na(as.data.frame(house_types) %>% filter(data$tipovivi5 == 1))) 
  #5600 tipovivi1: owned
  #153 tipovivi4: precarious
  #751 tipoivi5: assigned
  
  #assign 0 to NA in v2a1
  data$v2a1[is.na(data$v2a1)] = 0
  
  
  #v18q1 tablets owned:
  tabs = cbind(data$v18q, data$v18q1)
  # NA if zero, so replace with 0.
  data$v18q1[is.na(data$v18q1)] = 0
  
  
  #rez_esc: years behind in school
  school = as.data.frame(cbind(data$rez_esc, data$age))
  school_not_na = school %>% filter(!is.na(school$V1))
  #applies only to those in school (7-17)
  #fill with 0
  #in school binary
  data$in_school = as.numeric(!is.na(data$rez_esc))
  data$rez_esc[is.na(data$rez_esc)] = 0
  
  
  #meaneduc: avg edu for adults
  age18 = data %>% filter(age == 19 | age == 18)
  mean(na.omit(age18$meaneduc)) #13.66
  mean(na.omit(data$meaneduc)) #9.23
  #since small portion (5 obs), fill with mean for age (13.66)
  data$meaneduc[is.na(data$meaneduc)] = 13.66
  
  
  ### Standardize types ###
  #sapply(data, class) 
  
  #edjefe/edjefa
  #yes=1, no=0
  data$edjefa[data$edjefa == 'no'] = 0
  data$edjefa[data$edjefa == 'yes'] = 1
  data$edjefa = as.numeric(data$edjefa)
  
  data$edjefe[data$edjefe == 'no'] = 0
  data$edjefe[data$edjefe == 'yes'] = 1
  data$edjefe = as.numeric(data$edjefe)
  
  
  # #dependancy: (number of members of the household younger than 19 or older than 64)/(number of member of household between 19 and 64)         
  table(data$dependency)
  
  #calculate for missing vals (0 if none)
  dependency_calc = data %>%
    group_by(idhogar) %>%
    summarise(count_in_range = sum(age >= 19 & age <= 64),
              count_out_range = sum(!(age >= 19 & age <= 64)),
              ratio = ifelse(count_out_range == 0, 0, count_in_range / count_out_range))
  
  result = inner_join(data, dependency_calc, by = "idhogar")
  result = result %>% select(-c(count_in_range, count_out_range,dependency))
  result$dependency = result$ratio
  data = result %>% select(-c(ratio))
  
  
  
  ##### New Variables #####
  
  #new var for number of people in school by household
  school_count = data %>%
    group_by(idhogar) %>%
    summarise(school_count = sum(in_school == 1))
  
  data = inner_join(data, school_count, by = "idhogar")
  
  
  #count of children behind in school
  children_behind = data %>%
    group_by(idhogar) %>%
    summarise(children_behind = sum(rez_esc != 0))
  
  data = inner_join(data, children_behind, by = "idhogar")
  
  
  #count of disabled people in house
  disabled_count = data %>%
    group_by(idhogar) %>%
    summarise(dis_count = sum(dis == 1))
  
  data = inner_join(data, disabled_count, by = "idhogar")
  
  
  #has non-family in household binary
  non_fam = data %>%
    group_by(idhogar) %>%
    summarise(non_family = as.numeric(any(parentesco12 != 0)))
  
  data = inner_join(data, non_fam, by = "idhogar")
  
  
  #split ids
  numeric_data = data %>% select(-c(Id,idhogar,Target))
  
  #check correlations (spearman for nonlinear)
  # corrs = cor(numeric_data, method = 'spearman') %>%
  #           as.data.frame() %>%
  #           mutate(var1 = rownames(.)) %>%
  #           gather(var2, value, -var1) %>%
  #           arrange(desc(value)) %>%
  #           group_by(value) %>%
  #           filter(row_number()==1)
  # 
  # corrs = corrs[corrs$value >= .7 | corrs$value <= -.7,]
  
  #tamhog, tamviv, r4t3 basically same info
  # male/female, area1/area2, and abast... should be binary
  
  #remove more redundant data
  data = data %>% select(-c(female, area2, abastaguafuera, tamhog, tamviv, v18q,in_school))
  
  
  #which vars are not the same within households
  # independent_vars = data %>%
  #   group_by(idhogar) %>%
  #   summarize(across(everything(), ~ n_distinct(.) > 1)) %>%
  #   select(-idhogar) %>%
  #   summarize(across(everything(), any)) %>%
  #   pivot_longer(cols = everything(), names_to = "Column", values_to = "NotConstant") %>%
  #   filter(NotConstant) %>%
  #   pull(Column)
  # 
  # print(independent_vars)
  
  
  ##### Head of Household Data ####
  
  data_hh = data %>%
    filter(parentesco1 == 1) %>%
    select(-c(r4h1,r4h2,r4h3,r4m1,r4m2,r4m3,r4t2,r4t3,
              mobilephone,edjefe,edjefa,dis, #redundant
              parentesco1,parentesco2,parentesco3,parentesco4,parentesco5,
              parentesco6,parentesco7,parentesco8,parentesco9,parentesco10,
              parentesco11,parentesco12, #all are in parentesco1, so useless
              elimbasu5, estadocivil1, #all zero
              Id, idhogar
              ))
  
  #which(colSums(data_hh) == 0) #remove 0s
  
  ### House quality variable ###
  
  #pisonotiene: no floor
  #pareddes: waste walls
  #cielorazo=0: no ceiling
  #noelec
  #sanitario1: no toilet
  #energcocinar1: no kitchen
  #refrig=0: no refridgerator
  
  #epared1-3: wall quality
  #etecho1-3:roof quality
  #eviv: floor quality
  
  house_vars = data_hh %>%
    select(c(pisonotiene,pareddes,cielorazo,noelec,sanitario1,refrig,
             energcocinar1,epared1,epared2,epared3,etecho1,etecho2,
             etecho3,eviv1,eviv2,eviv3))
  
  #flip binary to make same as others
  house_vars$refrig = as.numeric(house_vars$refrig == 0)
  house_vars$cielorazo = as.numeric(house_vars$cielorazo == 0)
  
  #exterior vars
  house_vars$wall_qual = ifelse(house_vars$epared1 == 1, 0,
              ifelse(house_vars$epared2 == 1, .5,
              ifelse(house_vars$epared3 == 1, 1,999)))
  
  house_vars$roof_qual = ifelse(house_vars$etecho1 == 1, 0,
              ifelse(house_vars$etecho2 == 1, .5,
              ifelse(house_vars$etecho3 == 1, 1,999)))
  
  house_vars$floor_qual = ifelse(house_vars$eviv1 == 1, 0,
              ifelse(house_vars$eviv2 == 1, .5,
              ifelse(house_vars$eviv3 == 1, 1,999)))
  
  house_vars$exterior_qual = house_vars %>%
    select(roof_qual,wall_qual,floor_qual) %>%
    rowMeans()
  
  #interior vars
  house_vars$interior_qual = house_vars %>%
    select(pisonotiene,pareddes,cielorazo,noelec,sanitario1,energcocinar1,refrig) %>%
    rowMeans()
  
  house_vars$interior_qual = 1-house_vars$interior_qual
  
  
  house_vars = house_vars %>% select(-c(epared1,epared2,epared3,
                                        etecho1,etecho2,etecho3,
                                        eviv1,eviv2,eviv3))
  
  house_vars$house_qual = house_vars %>%
    select(interior_qual, exterior_qual) %>%
    rowMeans()
  
  #put back in full dataset
  data_hh$interior_qual = house_vars$interior_qual
  data_hh$exterior_qual = house_vars$exterior_qual
  data_hh$house_qual = house_vars$house_qual
  
  return(data_hh)
}

data = data_cleaner(read.csv('poverty.csv'))
test_data = data_cleaner(read.csv('poverty-test-blinded.csv')) #works on test data.
```

```{r, include=T,fig.dim = c(8, 4)}
ggplot(data, aes(x = dependency, y = house_qual)) +
  geom_point(alpha = 0.6, color = "blue") + 
  geom_smooth(method = "lm", color = "red", se = TRUE) +
  labs(
    title = "Dependency vs House Quality",
    x = "Dependency Ratio",
    y = "House Quality Score"
  ) +
  theme_minimal()
```

As we can see from this plot, as dependency ratio increases, the house quality seems to have a slightly larger spread which could indicate that having more dependents could have an impact on the quality of a household. Another argument that could be made is that people that have more dependents appear to be in a position to provide for these dependents since there seems to be an upward trend of the house quality score as the dependency ratio increases. Additionally, there could be a relationship between the education level and the house quality. To view this, another scatterplot was made.

```{r,include=T,fig.dim = c(8, 4)}
ggplot(aes(x = house_qual, y = meaneduc, color = dependency), data = data) +
  geom_point() +
  labs(x = "House Quality", y = "Average Education Level", title = "House Quality vs Average Education Level", color = "Dependency Ratio") +
  theme_minimal()
```

As we can see in this plot, the house quality appears to have a positive relationship with average education level. There is also a smaller dependency ratio as both house quality and education level rise.

\newpage


```{r}
#split train/test
set.seed(543)
train_ind = sample.int(dim(data)[1], dim(data)[1] * .7)
data_tr = data[train_ind,] #70% Training
data_te = data[-train_ind,] #30% Testing

#Undersampling 4's in training for model construction
tr_4<-which(data_tr$Target==4)
tr_3<-which(data_tr$Target==3)
tr_2<-which(data_tr$Target==2)
tr_1<-which(data_tr$Target==1)

set.seed(1)
keeps4 <- sample.int(length(tr_4), 150)
set.seed(1)
keeps3 <- sample.int(length(tr_3), 150)
set.seed(1)
keeps2 <- sample.int(length(tr_2), 150)
set.seed(1)
keeps1 <- sample.int(length(tr_1), 150)

undr<-c(tr_4[keeps4],tr_3[keeps3],tr_2[keeps2],tr_1[keeps1])

tr_undr <-data_tr[undr,]
```

```{r}
balmod = multinom(Target~., data=tr_undr)

uprobs = predict(balmod, data_te, type = "probs")
upreds = predict(balmod, data_te, type = "class")

confusionMatrix(as.factor(upreds), as.factor(data_te$Target)) #already decent
print(auc(multiclass.roc(as.numeric(data_te$Target), as.numeric(upreds))))
```


```{r}
# set.seed(1)
# ustep = step(multinom(Target~1, data=tr_undr),
#             scope=list(lower=multinom(Target~1, data=tr_undr),
#                        upper=multinom(Target~., data=tr_undr)),
#             direction="both", trace=0)
ustep <- multinom(formula = Target ~ escolari + r4t1 + energcocinar4 + 
    v18q1 + estadocivil5 + age + school_count + dependency + 
    hacapo + instlevel4 + estadocivil7 + qmobilephone + v2a1 + 
    etecho1 + pisomoscer + overcrowding, data = tr_undr)
ustep

uprobs = predict(ustep, data_te, type = "probs")
upreds = predict(ustep, data_te, type = "class")

confusionMatrix(as.factor(upreds), as.factor(data_te$Target)) #already decent
print(auc(multiclass.roc(as.numeric(data_te$Target), as.numeric(upreds))))

anova(balmod,ustep)
```





# Base Model

```{r}
#base model
print("#######")
mod0 = multinom(Target~., data=data_tr)
probs = predict(mod0, data_te, type = "probs")
preds = predict(mod0, data_te, type = "class")

confusionMatrix(as.factor(preds), as.factor(data_te$Target)) #already decent
print(auc(multiclass.roc(as.numeric(data_te$Target), as.numeric(preds))))

# #Takes a long time to run so after running it we just manually declare the models
# set.seed(1)
# backward <- step(multinom(Target~., data=data_tr),  direction="backward",
#                  trace=0, steps = 20)
# set.seed(1)
# forward = step(multinom(Target~1, data=data_tr), 
#                scope=list(lower=multinom(Target~1, data=data_tr), 
#                           upper=multinom(Target~., data=data_tr)), 
#                direction="forward", trace=0)
# set.seed(1)
# step = step(multinom(Target~1, data=data_tr), 
#             scope=list(lower=multinom(Target~1, data=data_tr), 
#                        upper=multinom(Target~., data=data_tr)), 
#             direction="both", trace=0)

#Backward model
print("#######")
backward <- multinom(Target~.-rooms- v14a- rez_esc- techoentrepiso- techootro- abastaguadentro- abastaguano- public- planpri- coopele- male- instlevel6- instlevel7- instlevel8- instlevel9- bedrooms- computer- television- school_count- non_family, data=data_tr)

anova(backward,mod0)[2,7]

bprobs = predict(backward, data_te, type = "probs")
bpreds = predict(backward, data_te, type = "class")

confusionMatrix(as.factor(bpreds), as.factor(data_te$Target)) #already decent
print(auc(multiclass.roc(as.numeric(data_te$Target), as.numeric(bpreds))))


#Forward/Stepwise Model
print("#######")
forward <- multinom(Target~meaneduc + hogar_nin + qmobilephone + house_qual + v2a1 + age + v18q1 + estadocivil5 + instlevel3 + area1 + paredblolad + epared1 + sanitario3 + estadocivil4 + techoentrepiso + hogar_adul + energcocinar2 + etecho1 + dis_count + children_behind + escolari + estadocivil2, data=data_tr)

anova(forward,mod0)[2,7]

fprobs = predict(forward, data_te, type = "probs")
fpreds = predict(forward, data_te, type = "class")

confusionMatrix(as.factor(fpreds), as.factor(data_te$Target)) #already decent
print(auc(multiclass.roc(as.numeric(data_te$Target), as.numeric(fpreds))))

#test various class weights
#Props: 8%, 16%, 12%, 64%

test_weight = function(probs,class_weights){
  weighted_probs = sweep(probs, 2, class_weights, `*`)
  weighted_probs = sweep(weighted_probs, 1, rowSums(weighted_probs), `/`)
  adjusted_preds = colnames(weighted_probs)[apply(weighted_probs, 1, which.max)]
  print(confusionMatrix(as.factor(adjusted_preds), as.factor(data_te$Target)))
  print(auc(multiclass.roc(as.numeric(data_te$Target), as.numeric(adjusted_preds))))
}

print("#######")
inv_prop = c(Class1 = 1/.08, Class2 = 1/.16, Class3 = 1/.12, Class4 = 1/.64)
test_weight(probs,inv_prop)

print("#######")
inv_sqrt = c(Class1 = 1/sqrt(.08), Class2 = 1/sqrt(.16), Class3 = 1/sqrt(.12), Class4 = 1/sqrt(.64))
test_weight(probs,inv_sqrt)
```

Our first step is to get a baseline model. Since our response is categorical with 4 levels, we will be using a multinomial logistic regression model. We separated  our data with a 70% train/test split and created a model using all variables created and retained in the data cleaning process. The performance of this model is shown below:

| **Base Model**  | 1    | 2    | 3    | 4    |
|-------------|------|------|------|------|
| Sensitivity | 0.22 | 0.31 | 0.05 | 0.91 |
| Specificity | 0.96 | 0.90 | 0.97 | 0.46 |

The initial model correctly classifies true non-vulnerable households quite well as shown by the high sensitivity in class 4. However because of the low specificity in class 4 there is potential that the model is overclassifying households into the non-vulnerable category. In the other classes the model correctly identifies true negatives very often as shown by the high specificities in classes 1, 2, and 3. The low sensitivity values in classes 1, 2, and 3 as well as the low specificity value in class 4 suggests that the model is overclassifying households as non-vulnerable to poverty.

```{r,eval=F}
library(glmnet)
#Convert training data to matrix format, required for the package
xx <- model.matrix(Target~., data_tr)
yy <- data_tr$Target #packages requires this to be numeric

#Fit the lasso logistic regression
#alpha=1 gives lasso (alpha=0 gives ridge)
#Can change the model measurement. Using AUC here:

thresh = seq(from = 0, to = 1, by = 0.05)
for(t in thresh){
  lasso.out <- cv.glmnet(xx, yy, family="multinomial", alpha=t, type.measure="class")
  coef(lasso.out, s=lasso.out$lambda.min)
  
  probs = predict(lasso.out, model.matrix(Target~., data_te),type='response')
  preds = predict(lasso.out, model.matrix(Target~., data_te),type='class')
  #table(data_te$Target)
  print(t)
  print(table(preds))
}

#all elastic net are not good.
```

To improve our model, we first attempted elastic net with crossfold validation (10 folds). We initially attempted ridge regression, but noticed that the model was only predicting cases from levels 2 and 4; the two largest groups within the data. We next tried LASSO regression, and had nearly identical results. Finally, in order to rule out elastic net as a viable strategy for our data, we tested every $\alpha$ value between 0 and 1 in intervals of 0.05. Out of these 20 tests, $\alpha=0.3$ was able to make predictions on levels 1, 2, and 4, $\alpha=0.7$ was able to make predictions on all 4 classes, and all others were only able to predict on 2 and 4. However, neither $\alpha=0.3$ nor $\alpha=0.7$ were able to make accurate predictions, so none of the elastic net models will be used. We also attempted Forward, Backward, and Stepwise selection. Backward selection dropped the variables rooms, v14a, rez_esc, techoentrepiso, techootro, abastaguadentro, abastaguano, public, planpri, coopele, male, instlevel6, instlevel7, instlevel8, instlevel9, bedrooms, computer, television, school_count, and non_family. Even with these missing variables it performed similarly to the full model. Forward selection only kept the variables meaneduc, hogar_nin, qmobilephone, house_qual, v2a1, age, v18q1, estadocivil5, instlevel3, area1, paredblolad, epared1, sanitario3, estadocivil4, techoentrepiso, hogar_adul, energcocinar2, etecho1, dis_count, children_behind, escolari, and estadocivil2. Even with significantly less variables it performed similarly to the full model. Stepwise selection yeilded the same model as forward selection. These three models performed similarly to the full model however the full model did result in all four vulnerability classifications having more balanced sensitivity values. Because of this we ruled these models out for now as we explored other options.


We hypothesized that, since the predictors appeared to not be a limiting factor, the class imbalance must be the reason for poor predictions. To account for this, we tested a few different weights on each of the class probability predictions. The weights, along with the performance of their corresponding models, are shown below:

| Weight                         | $\kappa$ | AUC  |
|--------------------------------|----------|------|
| None                           | 0.26     | 0.65 |
| $\frac{1}{\text{prop}}$        | 0.27     | .070 |
| $\frac{1}{\sqrt{\text{prop}}}$ | 0.31     | 0.68 |

As shown, both of these models have a larger $\kappa$ value and ROC-AUC scores which suggest they perform better then the unweighted model. The trade off, however, is that neither of these new models are nearly as accurate at predicting class 4 as the unweighted version as shown below.

| **Inverse Proportion Model**  | 1    | 2    | 3    | 4    |
|-------------|------|------|------|------|
| Sensitivity | 0.50 | 0.30 | 0.32 | 0.66 |
| Specificity | 0.88 | 0.87 | 0.82 | 0.83 |


| **Inverse Square Root Proportion Model**  | 1    | 2    | 3    | 4    |
|-------------|------|------|------|------|
| Sensitivity | 0.37 | 0.35 | 0.16 | 0.83 |
| Specificity | 0.93 | 0.88 | 0.92 | 0.65 |



\newpage

# Multi-Model Approach

Since our weighted model did so much better in predicting classes 1-3, we will continue to use it. However, to improve on its lacking capabilities in predicting class 4, we will introduce a second model which *exclusively* predicts class 4.

For this model, the Target variable has been transformed to a binary classifier as to whether the case falls into class 4 or not. In this model, we want as high of an specificity as we can reasonably get, since false positives won't be classified again by model 2. Since recall (the amount of true positives divided by the total predicted positives) appears to scale linearly with threshold, we took the highest threshold with an acceptable $\kappa$ and AUC score. This value ended up being 0.75. The results are shown in the plot below:

```{r}
##### Binary class 4 predictor #####
#get true/false if in group 4
data_tr_4 = data_tr
data_tr_4$Target = data_tr$Target == 4

data_te_4 = data_te
data_te_4$Target = data_te$Target == 4

#fit model
mod_4 = multinom(Target ~ ., data = data_tr_4)
probs_4 = predict(mod_4, data_te_4, type = "probs")

#test thresh
is = seq(from = 0, to = 1, by = 0.05)
ks = c()
aucs = c()
recs = c()
for(i in is){
  preds_4 = probs_4 > i
  
  conf_mat = confusionMatrix(as.factor(preds_4),as.factor(data_te_4$Target))
  ks = c(ks, conf_mat$overall['Kappa'])
  aucs = c(aucs, auc(roc(as.numeric(data_te_4$Target), as.numeric(preds_4))))
  recs = c(recs, conf_mat$byClass["Recall"])
}

results = data.frame(cbind(is,ks,aucs,recs))
results$ratio = results$ks/results$aucs

#use .75
preds_4 = probs_4 > .75
confusionMatrix(as.factor(preds_4), as.factor(data_te_4$Target))
```

```{r, include = T, fig.dim=c(8, 4)}
results2 = gather(results, key = "var", value = "y", ks:recs)

ggplot(results2, aes(x=is, y=y, colour=var))+
  geom_line(linewidth = 1.2)+
  geom_vline(xintercept = .75,linetype='dashed')+
  labs(x='Threshold', y='Value', title = 'Metrics for classifying non-vulnerable households by threshold')+
  scale_color_discrete(labels = c("AUC", "Kappa", "Recall"))+
  theme_minimal() +
  theme(legend.title = element_blank())
```

Now that we have a threshold selected for the binary model, all we need to do is pass the values that it predicts are not 4 to the original model. The results are shown in the following table:

```{r}
##### Combine models #####
final_preds = sapply(1:nrow(data_te), 
                     function(i) {
  # bin 4/not 4
  is_4_probs = predict(mod_4, newdata = data_te[i, ], type = "probs")
  is_4 = is_4_probs > .75
  
  if (is_4 == T) {
    return(4)
  }
  
  # predict on rest
  else {
    not_4_probs = matrix(predict(mod0, newdata = data_te[i, ], type = 'probs'),nrow = 1)
    class_weights = c(Class1=1/.08,Class2=1/.16,Class3=1/.12,Class4=1/.64)

    weighted_probs = sweep(not_4_probs, 2, class_weights, `*`)
    weighted_probs = sweep(weighted_probs, 1, rowSums(weighted_probs), `/`)
    not_4_pred = which.max(weighted_probs)
    
    return(not_4_pred)
  }
})

confusionMatrix(as.factor(final_preds), as.factor(data_te$Target))
```

| **Final Model**  | 1    | 2    | 3    | 4    |
|-------------|------|------|------|------|
| Sensitivity | 0.50 | 0.30 | 0.30 | 0.68 |
| Specificity | 0.88 | 0.87 | 0.84 | 0.80 |

While the sensitivity for observations in class 4 may have fallen by a decent margin, its specificity rose to compensate. While metrics for class 2 predictions are more or less the same, metrics for class 1 and 3 predictions have seen substantial improvements. This model shows a greater emphasis on correctly identifying those vulnerable to poverty. This contrasts against our initial model which over emphasized the correct identification of those not vulnerable to poverty. We believe the new model will suggest a better allocation of resources to those vulnerable to poverty than the intial model would have suggested.

# Conclusion

```{r,eval=F}
#get preds on blinded data
blind_preds = sapply(1:nrow(test_data), function(i) {
  # bin 4/not 4
  is_4_probs = predict(mod_4, newdata = test_data[i, ], type = "probs")
  is_4 = is_4_probs > .75
  
  if (is_4 == T) {
    return(4)
  }
  
  # predict on rest
  else {
    not_4_probs = matrix(predict(mod0, newdata = test_data[i, ], type = 'probs'),nrow = 1)
    class_weights = c(Class1=1/.08,Class2=1/.16,Class3=1/.12,Class4=1/.64)

    weighted_probs = sweep(not_4_probs, 2, class_weights, `*`)
    weighted_probs = sweep(weighted_probs, 1, rowSums(weighted_probs), `/`)
    not_4_pred = which.max(weighted_probs)
    
    return(not_4_pred)
  }
})

write.csv(blind_preds, 'predictions.csv')
```


